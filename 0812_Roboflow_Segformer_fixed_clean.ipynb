{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0600352a",
   "metadata": {
    "id": "SBpqCHWOUsdz"
   },
   "source": [
    "### ì²« ì½”ë“œì—ì„œ ì„¤ì¹˜ì™€, Semantic Segmentation Masks zipíŒŒì¼ì„ ì—…ë¡œë“œí–ˆê¸°ì— ì˜¤ë¥˜ê°€ ë– ë„ ì¼ë‹¨ ë‚¨ê²¨ë‘ì—ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf80741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 0) í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "# ========================\n",
    "!pip install -q \"transformers>=4.44,<5\" accelerate evaluate opencv-python-headless pillow\n",
    "\n",
    "# ========================\n",
    "# 1) Roboflow ZIP ì—…ë¡œë“œ\n",
    "# ========================\n",
    "from google.colab import files\n",
    "up = files.upload()  # Roboflowì—ì„œ ë°›ì€ dataset.zip ì„ íƒ\n",
    "ZIP_PATH = \"/content/\" + list(up.keys())[0]\n",
    "\n",
    "# ========================\n",
    "# 2) ì••ì¶• í•´ì œ\n",
    "# ========================\n",
    "import os, zipfile, shutil\n",
    "\n",
    "EXTRACT_DIR = \"/content/ds_rf\"\n",
    "if os.path.isdir(EXTRACT_DIR):\n",
    "    shutil.rmtree(EXTRACT_DIR)\n",
    "os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
    "    z.extractall(EXTRACT_DIR)\n",
    "\n",
    "print(\"unzipped to\", EXTRACT_DIR, \"->\", os.listdir(EXTRACT_DIR))\n",
    "\n",
    "# ========================\n",
    "# 3) ë°ì´í„° êµ¬ì¡° íŒŒì•…\n",
    "# ========================\n",
    "def find_split_dir(root, names=(\"train\",\"valid\",\"val\",\"test\")):\n",
    "    found = {}\n",
    "    for n in names:\n",
    "        p = os.path.join(root, n)\n",
    "        if os.path.isdir(p):\n",
    "            found[\"valid\" if n in (\"valid\",\"val\") else n] = p\n",
    "    return found\n",
    "\n",
    "splits = find_split_dir(EXTRACT_DIR)\n",
    "if not splits:\n",
    "    raise RuntimeError(\"train/valid/test í´ë”ë¥¼ ì°¾ì§€ ëª»í•¨. ZIP ë‚´ìš© í™•ì¸\")\n",
    "\n",
    "# ========================\n",
    "# 4) í•™ìŠµ í´ë˜ìŠ¤ ì„¤ì •\n",
    "# ========================\n",
    "COLLAPSE_TO_BINARY = True  # Trueë©´ ëª¨ë“  non-zero â†’ 'lane(1)'\n",
    "if COLLAPSE_TO_BINARY:\n",
    "    CLASS_NAMES = [\"background\", \"lane\"]\n",
    "else:\n",
    "    CLASS_NAMES = [\"background\", \"lane\", \"lane-dot\", \"lane-mid\", \"lane_crosswalk\"]\n",
    "\n",
    "id2label = {i: n for i, n in enumerate(CLASS_NAMES)}\n",
    "label2id = {n: i for i, n in id2label.items()}\n",
    "NUM_LABELS = len(CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00e3878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === íŒ¨ì¹˜: RFSegFolderë¥¼ ë” ê´€ëŒ€í•œ ë²„ì „ìœ¼ë¡œ ì¬ì •ì˜ ===\n",
    "import os, glob, re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# ì´ë¯¸ì§€/ë§ˆìŠ¤í¬ íŒŒì¼ëª… ë§¤ì¹­ì„ ìœ„í•´ ë’¤ì— ë¶™ëŠ” ì ‘ë¯¸ì–´ë“¤ì„ ì œê±°\n",
    "_SUFFIX_RE = re.compile(r'(_|-)(mask|masks|label|labels|seg|segment|segmentation)$', re.I)\n",
    "\n",
    "def _stem_no_suffix(path):\n",
    "    s = os.path.splitext(os.path.basename(path))[0]\n",
    "    s = _SUFFIX_RE.sub('', s)   # ..._mask, -labels ë“± ì œê±°\n",
    "    return s\n",
    "\n",
    "def _is_img(name):\n",
    "    return name.lower().endswith((\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\"))\n",
    "\n",
    "class RFSegFolder(Dataset):\n",
    "    def __init__(self, split_dir, processor):\n",
    "        # 1) ì´ë¯¸ì§€ í´ë” íƒìƒ‰: 'images/'ê°€ ìˆìœ¼ë©´ ê±°ê¸°, ì—†ìœ¼ë©´ split ë£¨íŠ¸ì—ì„œ ë°”ë¡œ ì°¾ê¸°\n",
    "        img_cands = [os.path.join(split_dir, \"images\"), split_dir]\n",
    "        self.img_dir = None\n",
    "        for d in img_cands:\n",
    "            if os.path.isdir(d) and any(_is_img(f) for f in os.listdir(d)):\n",
    "                self.img_dir = d\n",
    "                break\n",
    "        if self.img_dir is None:\n",
    "            raise RuntimeError(f\"No images found in {split_dir}\")\n",
    "\n",
    "        # 2) ë§ˆìŠ¤í¬ í´ë” í›„ë³´: labels/masks/annotations/â€¦ ì—†ìœ¼ë©´ split ë£¨íŠ¸ê¹Œì§€ í¬í•¨\n",
    "        mask_cands = [\"masks\",\"labels\",\"annotations\",\"masks_png\",\"labels_png\",\"mask\",\"Labels\",\"Masks\"]\n",
    "        self.mask_dirs = [os.path.join(split_dir, c) for c in mask_cands if os.path.isdir(os.path.join(split_dir, c))]\n",
    "        if not self.mask_dirs:\n",
    "            # ë§ˆì§€ë§‰ ìˆ˜ë‹¨: split ë””ë ‰í† ë¦¬ ì•ˆì—ì„œ PNGê°€ ìˆëŠ” ëª¨ë“  í´ë”ë¥¼ ìŠ¤ìº”(ì´ë¯¸ì§€ í´ë” ì œì™¸)\n",
    "            self.mask_dirs = []\n",
    "            for root, dirs, files in os.walk(split_dir):\n",
    "                if os.path.abspath(root) == os.path.abspath(self.img_dir):\n",
    "                    continue\n",
    "                if any(f.lower().endswith(\".png\") for f in files):\n",
    "                    self.mask_dirs.append(root)\n",
    "            if not self.mask_dirs:\n",
    "                # ì •ë§ ì—†ìœ¼ë©´ ë£¨íŠ¸ë„ í›„ë³´ì— í¬í•¨(ì•„ì£¼ ë“œë¬¸ ì¼€ì´ìŠ¤)\n",
    "                self.mask_dirs = [split_dir]\n",
    "\n",
    "        self.processor = processor\n",
    "\n",
    "        # 3) ë§ˆìŠ¤í¬ ì¸ë±ìŠ¤ êµ¬ì¶• (ë™ì¼ stem ë§¤ì¹­)\n",
    "        mask_map = {}\n",
    "        for md in self.mask_dirs:\n",
    "            for p in glob.glob(os.path.join(md, \"*.png\")):\n",
    "                mask_map[_stem_no_suffix(p)] = p\n",
    "\n",
    "        # 4) ì´ë¯¸ì§€-ë§ˆìŠ¤í¬ í˜ì–´ ë§Œë“¤ê¸°\n",
    "        self.items = []\n",
    "        for ip in sorted(glob.glob(os.path.join(self.img_dir, \"*.*\"))):\n",
    "            if not _is_img(ip):\n",
    "                continue\n",
    "            st = _stem_no_suffix(ip)\n",
    "            mp = mask_map.get(st)\n",
    "            if mp and os.path.exists(mp):\n",
    "                self.items.append((ip, mp))\n",
    "\n",
    "        if not self.items:\n",
    "            # ë””ë²„ê¹… ë„ì›€: í´ë” ì•ˆì— ë­ê°€ ìˆëŠ”ì§€ ì¡°ê¸ˆ ì°ì–´ì¤Œ\n",
    "            print(\"[DEBUG] img_dir:\", self.img_dir)\n",
    "            print(\"[DEBUG] mask_dirs:\", self.mask_dirs[:3], \"â€¦\", f\"({sum(len(glob.glob(os.path.join(d,'*.png'))) for d in self.mask_dirs)} masks png)\")\n",
    "            raise RuntimeError(f\"No (image,mask) pairs in {split_dir}. \"\n",
    "                               f\"ì´ë¯¸ì§€/ë§ˆìŠ¤í¬ íŒŒì¼ëª…ì´ ì„œë¡œ ë§¤ì¹­ë˜ëŠ”ì§€(ì˜ˆ: abc.jpg â†” abc_mask.png) í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ip, mp = self.items[idx]\n",
    "        image = Image.open(ip).convert(\"RGB\")\n",
    "        # íŒ”ë ˆíŠ¸/ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ëª¨ë‘ ì§€ì›: 0=ë°°ê²½, 1+=ì „ë¶€ ì°¨ì„ ìœ¼ë¡œ ë­‰ì¹˜ê¸°(ì´ì§„)\n",
    "        m = np.array(Image.open(mp).convert(\"L\"), dtype=np.uint8)\n",
    "        m = (m > 0).astype(np.uint8)  # ì´ì§„ ì„¸íŒ… (ì—¬ëŸ¬ í´ë˜ìŠ¤ë¥¼ ì“°ë ¤ë©´ ì—¬ê¸° ë¡œì§ ë°”ê¿”ë„ ë¨)\n",
    "        enc = processor(images=image, segmentation_maps=m, return_tensors=\"pt\")\n",
    "        return {k: v.squeeze(0) for k, v in enc.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a931cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 5) í”„ë¡œì„¸ì„œ/ëª¨ë¸ ë¡œë“œ\n",
    "# ========================\n",
    "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n",
    "import torch\n",
    "\n",
    "CKPT = \"nvidia/segformer-b0-finetuned-ade-512-512\"\n",
    "\n",
    "processor = SegformerImageProcessor.from_pretrained(\n",
    "    CKPT,\n",
    "    reduce_labels=False  # ë¼ë²¨ ì¤„ì„ ë¹„í™œì„±í™”(ìš°ë¦¬ í´ë˜ìŠ¤ ì¸ë±ìŠ¤ ìœ ì§€)\n",
    ")\n",
    "\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    CKPT,\n",
    "    num_labels=NUM_LABELS,   # ìœ„ì—ì„œ ë§Œë“  NUM_LABELS ì‚¬ìš©\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True  # í´ë¼ìŠ¤ ìˆ˜ê°€ ë‹¬ë¼ ìƒê¸°ëŠ” shape mismatch í—ˆìš©\n",
    ")\n",
    "\n",
    "# ========================\n",
    "# 6) ë°ì´í„°ì…‹ ìƒì„±\n",
    "# ========================\n",
    "# splitsëŠ” ì´ë¯¸ ìœ„ì—ì„œ ë§Œë“¤ì–´ ë‘” dict: {'train': ..., 'valid': ..., 'test': ...} ì¤‘ ì¼ë¶€\n",
    "train_dir = splits.get(\"train\")\n",
    "valid_dir = splits.get(\"valid\") or splits.get(\"val\") or train_dir  # valid ì—†ìœ¼ë©´ train ì¬ì‚¬ìš©\n",
    "\n",
    "if train_dir is None:\n",
    "    raise RuntimeError(\"train í´ë”ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ZIP êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "train_ds = RFSegFolder(train_dir, processor)  # ì´ì§„ ì„¸íŒ…ì€ í´ë˜ìŠ¤ ë‚´ë¶€ì—ì„œ m>0 â†’ 1ë¡œ ì²˜ë¦¬\n",
    "val_ds   = RFSegFolder(valid_dir, processor)\n",
    "print(f\"âœ… Dataset ready: train={len(train_ds)}, valid={len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049c0448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 7) í•™ìŠµ + ì €ì¥ + ë‹¤ìš´ë¡œë“œ\n",
    "# ========================\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np, evaluate, torch, os, shutil, zipfile\n",
    "from google.colab import files\n",
    "\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "\n",
    "def _to_py(o):\n",
    "    import numpy as np\n",
    "    if isinstance(o, np.ndarray):\n",
    "        return o.tolist()\n",
    "    if isinstance(o, (np.floating, np.integer)):\n",
    "        return o.item()\n",
    "    return o\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred  # logits: (N, C, h, w), labels: (N, H, W)\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    lt = torch.from_numpy(logits)\n",
    "    yt = torch.from_numpy(labels)\n",
    "\n",
    "    # ë¼ë²¨ í¬ê¸°ì— ë§ì¶° ì—…ìƒ˜í”Œ(í¬ê¸° ë¶ˆì¼ì¹˜ ë°©ì§€)\n",
    "    lt_up = torch.nn.functional.interpolate(\n",
    "        lt, size=yt.shape[-2:], mode=\"bilinear\", align_corners=False\n",
    "    )\n",
    "    preds = lt_up.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "    res = metric.compute(\n",
    "        predictions=preds,\n",
    "        references=labels,\n",
    "        num_labels=NUM_LABELS,\n",
    "        ignore_index=255,\n",
    "        reduce_labels=False,\n",
    "    )\n",
    "    return {k: _to_py(v) for k, v in res.items()}\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"segformer-lane\",\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    eval_strategy=\"epoch\",   # transformers 4.xëŠ” evaluation_strategy ì‚¬ìš©\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"mean_iou\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# í•™ìŠµ\n",
    "trainer.train()\n",
    "\n",
    "# ë² ìŠ¤íŠ¸ ì €ì¥\n",
    "BEST_DIR = \"segformer-lane/best\"\n",
    "os.makedirs(BEST_DIR, exist_ok=True)\n",
    "trainer.save_model(BEST_DIR)            # ëª¨ë¸ ê°€ì¤‘ì¹˜/êµ¬ì„±\n",
    "processor.save_pretrained(BEST_DIR)     # í”„ë¡œì„¸ì„œ\n",
    "\n",
    "print(\"âœ… Saved best to:\", BEST_DIR)\n",
    "\n",
    "# ì•„í‹°íŒ©íŠ¸ ì••ì¶•(zip) í›„ ë‹¤ìš´ë¡œë“œ(í•„ìš”í•œ ê²ƒë§Œ ë¬¶ìŒ)\n",
    "ZIP_OUT = \"segformer_lane_best.zip\"\n",
    "with zipfile.ZipFile(ZIP_OUT, \"w\", zipfile.ZIP_DEFLATED) as z:\n",
    "    # í•µì‹¬ íŒŒì¼ë“¤ë§Œ ì„ íƒ ì €ì¥\n",
    "    for fname in [\n",
    "        \"config.json\", \"preprocessor_config.json\", \"model.safetensors\", \"pytorch_model.bin\"\n",
    "    ]:\n",
    "        p = os.path.join(BEST_DIR, fname)\n",
    "        if os.path.exists(p):\n",
    "            z.write(p, arcname=os.path.join(\"best\", fname))\n",
    "    # trainer args/ë¡œê·¸ ë“± ë©”íƒ€(ì„ íƒ)\n",
    "    for extra in [\"trainer_state.json\", \"trainer_config.json\", \"all_results.json\"]:\n",
    "        p = os.path.join(\"segformer-lane\", extra)\n",
    "        if os.path.exists(p):\n",
    "            z.write(p, arcname=os.path.join(\"run_meta\", extra))\n",
    "\n",
    "print(\"ğŸ“¦ Zip created:\", ZIP_OUT)\n",
    "\n",
    "# ë‹¤ìš´ë¡œë“œ íŠ¸ë¦¬ê±°\n",
    "files.download(ZIP_OUT)  # Colabì—ì„œ íŒŒì¼ ë‹¤ìš´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c0f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â–¶ ì˜ìƒ ì—…ë¡œë“œ\n",
    "from google.colab import files\n",
    "up = files.upload()  # ë¡œì»¬ì—ì„œ mp4 ë“± ì„ íƒ\n",
    "VIDEO_IN = \"/content/\" + list(up.keys())[0]\n",
    "print(\"ì…ë ¥ ì˜ìƒ:\", VIDEO_IN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d6d188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â–¶ SegFormer ì¶”ë¡  + ì»¬ëŸ¬ ì˜¤ë²„ë ˆì´\n",
    "import os, cv2, json, numpy as np, torch\n",
    "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n",
    "\n",
    "# ===== ê²½ë¡œ ì„¤ì • =====\n",
    "MODEL_DIR = \"/content/segformer-lane/best\"  # í•™ìŠµ ì €ì¥í•œ ë””ë ‰í„°ë¦¬\n",
    "VIDEO_OUT = \"/content/out_lane_overlay.mp4\"\n",
    "ALPHA = 0.5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ===== ëª¨ë¸/í”„ë¡œì„¸ì„œ ë¡œë“œ =====\n",
    "processor = SegformerImageProcessor.from_pretrained(MODEL_DIR)\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(MODEL_DIR)\n",
    "model.to(DEVICE).eval()\n",
    "\n",
    "# ===== ë¼ë²¨ ì´ë¦„ & ìƒ‰ìƒ ê³ ì • =====\n",
    "label_names = [\"background\", \"lane\", \"lane_dot\", \"lane_mid\", \"lane_crosswalk\"]\n",
    "\n",
    "# OpenCV BGR ìƒ‰ìƒ (ë°°ê²½ ì œì™¸)\n",
    "PALETTE = [\n",
    "    (0, 0, 0),           # background\n",
    "    (0, 255, 0),         # lane         - ì´ˆë¡\n",
    "    (0, 165, 255),       # lane_dot     - ì£¼í™©\n",
    "    (255, 0, 0),         # lane_mid     - íŒŒë‘\n",
    "    (255, 255, 255)      # crosswalk    - í°ìƒ‰\n",
    "]\n",
    "num_labels = len(PALETTE)\n",
    "\n",
    "print(\"Label names:\", label_names)\n",
    "print(\"Num labels:\", num_labels)\n",
    "\n",
    "# ===== ë¹„ë””ì˜¤ IO =====\n",
    "cap = cv2.VideoCapture(VIDEO_IN)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(f\"Cannot open video: {VIDEO_IN}\")\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "w   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h   = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "out = cv2.VideoWriter(VIDEO_OUT, fourcc, fps, (w, h))\n",
    "\n",
    "# ===== ì¶”ë¡  ë£¨í”„ =====\n",
    "with torch.no_grad():\n",
    "    frame_idx = 0\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        inputs = processor(images=rgb, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        logits = torch.nn.functional.interpolate(\n",
    "            logits, size=(h, w), mode=\"bilinear\", align_corners=False\n",
    "        )\n",
    "        pred = logits.argmax(dim=1)[0].detach().cpu().numpy().astype(np.int32)\n",
    "\n",
    "        # ===== ì»¬ëŸ¬ ì˜¤ë²„ë ˆì´ =====\n",
    "        overlay = np.zeros_like(frame)\n",
    "        for cls_id in range(1, num_labels):  # 0=ë°°ê²½ì€ ì œì™¸\n",
    "            mask = (pred == cls_id)\n",
    "            if mask.any():\n",
    "                overlay[mask] = PALETTE[cls_id]\n",
    "\n",
    "        blended = cv2.addWeighted(frame, 1.0 - ALPHA, overlay, ALPHA, 0.0)\n",
    "        out.write(blended)\n",
    "\n",
    "        frame_idx += 1\n",
    "        if frame_idx % 50 == 0:\n",
    "            print(f\"Processed {frame_idx} frames...\")\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(\"âœ… Saved video:\", VIDEO_OUT)\n",
    "\n",
    "# ê²°ê³¼ ë‹¤ìš´ë¡œë“œ\n",
    "files.download(VIDEO_OUT)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
